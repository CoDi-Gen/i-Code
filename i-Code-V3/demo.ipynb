{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ContentList'></a>\n",
    "# Content List\n",
    "\n",
    "## Single to Single Generation \n",
    "\n",
    "### 1. [Text To Image](#TextToImage)\n",
    "\n",
    "### 2. [Image To Text](#ImageToText)\n",
    "\n",
    "### 3. [Text To Audio](#TextToAudio)\n",
    "\n",
    "### 4. [Audio To Text](#AudioToText)\n",
    "\n",
    "### 5. [Image To Audio](#ImageToAudio)\n",
    "\n",
    "### 6. [Audio To Image](#AudioToImage)\n",
    "\n",
    "### 7. [Text To Video](#TextToVideo)\n",
    "\n",
    "## Multi-Conditioning Generation\n",
    "\n",
    "### 1. [Text + Image + Audio To Image](#TextImageAudioToImage)\n",
    "\n",
    "## Joint Multimodal Generation\n",
    "\n",
    "### 1. [Text To Image+Text](#TextToImageText)\n",
    "\n",
    "### 1. [Text To Video+Audio](#TextToImageText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadModel'></a>\n",
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from core.models.model_module_infer import model_module\n",
    "\n",
    "model_load_path = 'model_alldms_sd_vd_largeaudioenc.pth'\n",
    "inference_tester = model_module(data_dir='/data1/terrantang/mmdif-data/', pth=model_load_path)\n",
    "inference_tester = inference_tester.cuda()\n",
    "inference_tester = inference_tester.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToImage'></a>\n",
    "# Text To Image\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Give a prompt\n",
    "prompt = \"Surfer on his surfboard in a wave\"\n",
    "\n",
    "# Generate image\n",
    "n_samples = 1\n",
    "images = inference_tester.inference(\n",
    "                xtype = 'image',\n",
    "                ctx = prompt,\n",
    "                n_samples = 1, \n",
    "                image_size = 512)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToAudio'></a>\n",
    "# Text To Audio\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Give a prompt\n",
    "prompt = 'heavy raining.'\n",
    "\n",
    "# Generate audio\n",
    "audio_spec = inference_tester.inference(\n",
    "                xtype = 'audio',\n",
    "                cin = prompt,\n",
    "                ctype = 'prompt',\n",
    "                scale = 7.5,\n",
    "                n_samples = 1, \n",
    "                ddim_steps = 50)\n",
    "\n",
    "audio_wavs = inference_tester.mel_spectrogram_to_waveform(audio_spec[0])\n",
    "\n",
    "# Visualize audio and play\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(audio_spec[0].squeeze().transpose(0,1).cpu().numpy()[:, :])\n",
    "plt.show()\n",
    "from IPython.display import Audio\n",
    "Audio(audio_wavs.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImageToAudio'></a>\n",
    "# Image To Audio\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "from PIL import Image\n",
    "from core.common.utils import regularize_image\n",
    "im = Image.open('./assets/demo_files/rain_on_tree.jpg')\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audio\n",
    "n_samples = 1\n",
    "audio_spec = inference_tester.inference(\n",
    "                xtype = 'audio',\n",
    "                cin = im,\n",
    "                ctype = 'vision',\n",
    "                scale = 7.5,\n",
    "                n_samples = n_samples, \n",
    "                ddim_steps = 50)\n",
    "\n",
    "audio_wavs = inference_tester.mel_spectrogram_to_waveform(audio_spec)\n",
    "\n",
    "# Visualize the audio and play\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(audio_spec.squeeze().transpose(0,1).cpu().numpy()[:, :512])\n",
    "plt.show()\n",
    "from IPython.display import Audio\n",
    "Audio(audio_wavs.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AudioToImage'></a>\n",
    "# Audio To Image\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input audio andplay\n",
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "pad_time = 10.23\n",
    "\n",
    "path = './assets/demo_files/wind_chimes.wav'\n",
    "\n",
    "audio_wavs, sr = torchaudio.load(path)\n",
    "audio_wavs = torchaudio.functional.resample(waveform=audio_wavs, orig_freq=sr, new_freq=16000).mean(0)[:int(16000 * pad_time)]\n",
    "padding = torch.zeros([int(16000 * pad_time) - audio_wavs.size(0)])\n",
    "audio_wavs = torch.cat([audio_wavs, padding], 0)\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(path, rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image\n",
    "n_samples = 1\n",
    "images = inference_tester.inference(\n",
    "                xtype = 'image',\n",
    "                cin = audio_wavs,\n",
    "                ctype = 'audio',\n",
    "                scale = 7.5,\n",
    "                n_samples = n_samples, \n",
    "                image_size = 512,\n",
    "                ddim_steps = 50)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImageToText'></a>\n",
    "# Image To Text\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load an image input\n",
    "from PIL import Image\n",
    "im = Image.open('./assets/demo_files/cat.jpg')\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "text = inference_tester.inference(\n",
    "                xtype = 'text',\n",
    "                cin = im,\n",
    "                ctype = 'vision',\n",
    "                n_samples = n_samples, \n",
    "                ddim_steps = 50,\n",
    "                scale = 7.5,)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AudioToText'></a>\n",
    "# Audio To Text\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "path = './assets/demo_files/train_sound.flac'\n",
    "\n",
    "audio_wavs, sr = torchaudio.load(path)\n",
    "audio_wavs = torchaudio.functional.resample(waveform=audio_wavs, orig_freq=sr, new_freq=16000).mean(0)[:int(16000 * 10.23)]\n",
    "Audio(audio_wavs.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "text = inference_tester.inference(\n",
    "                xtype = 'text',\n",
    "                cin = audio_wavs,\n",
    "                ctype = 'audio',\n",
    "                n_samples = n_samples, \n",
    "                ddim_steps = 50,\n",
    "                scale = 7.5)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextImageAudioToImage'></a>\n",
    "#  Text + Image + Audio To Image\n",
    "\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Audio Inputs\n",
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "path = './assets/demo_files/classic_music.flac'\n",
    "\n",
    "audio_wavs, sr = torchaudio.load(path)\n",
    "audio_wavs = torchaudio.functional.resample(waveform=audio_wavs, orig_freq=sr, new_freq=16000).mean(0)[:int(16000 * 10.23)]\n",
    "Audio(audio_wavs.squeeze(), rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'dawn, dawn scenery, beautiful lighting.'\n",
    "\n",
    "# Load Image Inputs\n",
    "from PIL import Image\n",
    "im = Image.open('./assets/demo_files/van_gogh_image.jpg').resize((512, 512))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image\n",
    "n_samples = 1\n",
    "images = inference_tester.application_dualguided(\n",
    "                'image',\n",
    "                cad = audio_wavs,\n",
    "                ctx = prompt,\n",
    "                cim = im,\n",
    "                n_samples = n_samples,\n",
    "                image_size = 512,\n",
    "                mixing = 0.25,\n",
    "                mixing_c2 = 0.45,)\n",
    "\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextImageAudioToImage'></a>\n",
    "#  Text To Image + Text\n",
    "\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'deep diving in coral reef underwater.'\n",
    "\n",
    "outputs = inference_tester.application_dualguided(\n",
    "                'image',\n",
    "                cad = audio_wavs,\n",
    "                ctx = prompt,\n",
    "                cim = im,\n",
    "                n_samples = 1,\n",
    "                image_size = 512)\n",
    "\n",
    "image, text = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextImageAudioToImage'></a>\n",
    "#  Text To Video + Audio\n",
    "\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'deep diving in coral reef underwater.'\n",
    "\n",
    "\n",
    "n_samples = 1\n",
    "outputs = inference_tester.application_dualguided(\n",
    "                ['video', 'audio'],\n",
    "                ctx = prompt,\n",
    "                n_samples = 1,\n",
    "                image_size = 256,\n",
    "                ddim_steps = 50,\n",
    "                num_frames = 8,\n",
    "                scale = 7.5)\n",
    "\n",
    "video, audio_spec = outputs[0]\n",
    "\n",
    "\n",
    "audio_wavs = inference_tester.mel_spectrogram_to_waveform(audio_spec)\n",
    "# Visualize the audio and play\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(audio_spec.squeeze().transpose(0,1).cpu().numpy()[:, :512])\n",
    "plt.show()\n",
    "from IPython.display import Audio\n",
    "Audio(audio_wavs.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual video as gif\n",
    "from PIL import Image\n",
    "frame_one = video[0]\n",
    "path = \"./generated_video.gif\"\n",
    "frame_one.save(path, format=\"GIF\", append_images=video[1:],\n",
    "               save_all=True, duration=2000/len(video), loop=0)\n",
    "\n",
    "from IPython import display \n",
    "from IPython.display import Image\n",
    "Image(data=open(path,'rb').read(), format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
